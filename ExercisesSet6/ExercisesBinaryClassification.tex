\documentclass[]{article}

%opening
\title{Exercises Set 6}
\author{Paul Dubois}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stmaryrd}
\usepackage{graphicx}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Primes}{\mathbb{P}}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\txtand}{\text{ and }}
\newcommand{\txtor}{\text{ or }}
\newcommand{\lxor}{\veebar}


\begin{document}
	
	\maketitle
	
	\begin{abstract}
		As this is the last session, there will be no compulsory questions this time.
	\end{abstract}	
	
	\section{Lagrangian multiplier technique}
	\begin{center}
		\includegraphics[height=4cm]{Lagrange}
	\end{center}

	\subsection{Unconstrained optimization}
	Let $f(x,y) = 2x^2-12x + 4y^2 + 8y + 20$.\\
	Find $(x^*,y^*) \in \R^2$ such that $f$ reaches its minimum (i.e. $f(x^*,y^*) \leq f(x,y) \quad \forall (x,y) \in \R^2$).
	
	\subsection{(Equality) Constrained optimization}
	Let $f(x,y) = 2x^2-12x + 4y^2 + 8y + 20$.\\
	Suppose further that we want $3x + 5y = 2$.\\
	Find $(x^*,y^*) \in \R^2$ such that $3x^* + 5y^* = 2$ and $f$ reaches its minimum (i.e. $f(x^*,y^*) \leq f(x,y) \quad \forall (x,y) \in \R^2,\ 3x + 5y = 2$).
	
	\subsection{Lagrange multiplier}
	Let $f(x,y) = 2x^2-12x + 4y^2 + 8y + 20$.\\
	Suppose further that we want $3x + 5y = 2$.\\
	Let $\mathcal{L}(x,y,\lambda) = f(x,y) - \lambda (3x +5y - 2)$.\\
	Find the point where $\nabla \cdot \mathcal{L} = 0$
	
	\subsection{(Inequality) Constrained optimization}
	Let $f(x) = x^2-2x$.\\
	Suppose further that we want $3x \leq 2$.\\
	Find $x^* \in \R$ such that $3x^* \leq 2$ and $f$ reaches its minimum (i.e. $f(x^*) \leq f(x) \quad \forall x \in \R,\ 3x \leq 2$).
	\vspace{0.5cm}
	\\
	Let $f(x) = x^2+2x$.\\
	Suppose further that we want $3x \leq 2$.\\
	Find $x^* \in \R$ such that $3x^* \leq 2$ and $f$ reaches its minimum (i.e. $f(x^*) \leq f(x) \quad \forall x \in \R,\ 3x \leq 2$).
	
	\subsection{Lagrange multiplier}
	Let $f(x) = x^2-2x$.\\
	Suppose further that we want $3x \leq 2$.\\
	Let $\mathcal{L}(x,\lambda) = f(x) - \lambda (3x - 2)$.\\
	Find the point where $\nabla \cdot \mathcal{L} = 0$ and $\lambda \geq 0$.
	\vspace{0.5cm}
	\\
	Let $f(x) = x^2+2x$.\\
	Suppose further that we want $3x \leq 2$.\\
	Let $\mathcal{L}(x,\lambda) = f(x) - \lambda (3x - 2)$.\\
	Find the point where $\nabla \cdot \mathcal{L} = 0$ and $\lambda \geq 0$.
	
	
	
	
	
	
	\section{Support Vector Machines}
	\subsection{Theory}
	Define a line in $\R^2$ with parameters $\mathbf{w}$ and $b$ defined by $\mathbf{w}.\mathbf{x} = b$ (or $\mathbf{w}.\mathbf{x} - b = 0$) for $\mathbf{x} \in \R^2$.
	This line cut the plane in 2 regions:
	\begin{itemize}
		\item $\mathbf{x} \in \R^2$ such that $\mathbf{w}.\mathbf{x} - b < 0$
		\item $\mathbf{x} \in \R^2$ such that $\mathbf{w}.\mathbf{x} - b > 0$
	\end{itemize}
	The goal is to find $\mathbf{w}$ and $b$ such that all points of the first class are in the first region, and all points of the second class are in the second region.
	
	
	
	
	\subsection{Practice}
	The training dataset consists of the following data points:
	
	Positive class ("+1"):
	\begin{itemize}
		\item (2, 2)
		\item (1, 1)
	\end{itemize}
	
	Negative class ("-1"):
	\begin{itemize}
		\item (0, 1)
		\item (1, 0)
	\end{itemize}
	
	The SVM model's objective function for a linearly separable dataset is as follows:
	
	\[
	\text{Minimize } \frac{1}{2}\|\mathbf{w}\|^2
	\]
	
	Subject to the constraints:
	
	\[
	y_i(\mathbf{w}\cdot\mathbf{x}_i + b) \geq 1 \text{ for all } i
	\]
	
	Where:
	\begin{align*}
		y_i & \text{ is the class label } (+1 \text{ or } -1) \text{ of the } i\text{-th data point.} \\
		\mathbf{w} & \text{ is the weight vector normal to the hyperplane.} \\
		\mathbf{x}_i & \text{ is the } i\text{-th data point.} \\
		b & \text{ is the bias term.}
	\end{align*}
	
	\begin{enumerate}
		\item Calculate the optimal values of \(\mathbf{w}\) and \(b\) to separate the data points while maximizing the margin.
		\item Determine the equation of the optimal hyperplane in the form \(\mathbf{w}\cdot\mathbf{x} + b = 0\).
		\item Identify the support vectors in the dataset.
		\item Calculate the margin, which is the perpendicular distance from the hyperplane to the nearest support vector.
		\item Classify a new data point, \((3, 2)\), based on the learned SVM model.
	\end{enumerate}
	
	
	
	
	\section{Logistic Regression}
	You are working on a binary classification problem where you are using logistic regression to predict whether a student will pass (1) or fail (0) an exam based on the number of hours they have studied. You have collected the data in the provided table.
	
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|}
			\hline
			Hours Studied ($X$) & Pass ($Y$) \\
			\hline
			3 & 0 \\
			4 & 0 \\
			5 & 1 \\
			6 & 0 \\
			7 & 1 \\
			8 & 1 \\
			9 & 1 \\
			\hline
		\end{tabular}
	\end{table}
	
	You want to fit a logistic regression model to this data and find the best-fitting sigmoid function, which is represented as:
	$$P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X)}}$$
	Where $P(Y=1|X)$ is the probability of passing the exam given the hours studied.
	
	\begin{enumerate}
		%\item Calculate the log-odds for each data point, which is $\log\left(\frac{P(Y=1|X)}{1-P(Y=1|X)}\right)$.
		\item Calculate the cost function (log loss) for the given data and the predicted probabilities. The log loss for a single data point is given as:
		$$\text{Log Loss} = -\left(Y \cdot \log(P(Y=1|X)) + (1 - Y) \cdot \log(1 - P(Y=1|X))\right)$$
		\item Calculate the total cost (log loss) summing for all data points.
		\item Use gradient descent or any other optimization method to find the values of $\beta_0$ and $\beta_1$ that minimize the total cost; you may use a computer to perform gradient descent.
		\item Calculate the probability $P(Y=1|X=10)$ using the logistic regression model.
	\end{enumerate}
	
	
\end{document}
